---
layout:     post
title:      "初探机器学习的语义分词"
subtitle:   "中文分词组件-jieba"
date:       "2017-05-31"
author:     "TuoX"
header-img: "img/post-8/bg.png"
---

<p>今天跟大家介绍的是一个相对而言比较好用的分词组件-jieba.</p>
<p>对于中文的分词作用十分广泛，不仅仅是在搜索，尤其在机器学习上面也是非常的突出.</p>
<p>最近在写一个微信机器人，光光靠自己写的一些分词，已经完全不够用了，于是就开始寻找方案.目前比较流行的有结巴（jieba）分词与中科院（NLPIR）分词.</p>
<p>我经过一系列的权衡比较，选择了jieba.</p>
<p>我们先来看看jieba基于的算法吧.</p>
<ul>
<li>
基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)
</li>
<li>
采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合
</li>
<li>
对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法
</li>
</ul>
<p>以上的算法说起来今天一整天都会说不完，喜欢了解的小伙伴可以google查看，或者直接查看源码，进行进一步的了解.</p>
<p>这里就简单的介绍一下它的使用方法.</p>
```python
import jieba

seg_list = jieba.lcut_for_search(u"拖鞋帮我查下杭州到厦门的机票")
print("|".join(seg_list))
```
<img src="/img/post-8/jieba-result.png" />
<p>结果如上</p>
<p>分词分出来了，那我们接下来要做什么呢.当然是让我们的机器人去理解这些的意义.这里使用一个比较简单的方法来理解这段意思.</p>
<p>这段命令的重点在于机票，其次是出发地、目的地，最后是其他条件，比如时间：今天还是明天还是更远的明天.</p>
<p>取出这几个因素我是用这种方法，首先一个人正常人说一句话，都是习惯性的说从哪里到哪里，或者直接说到哪里，这里面"到"这个字是肯定不会少的.那我们就可以以这个到为分割点，取前面一个词与后面一个词语.有的小伙伴可能会说那不说前面的地点不就造成异常了嘛，确实是的，所以我们代码上面需要一些控制，采取必要的策略.比如获取该用户目前的地点作为默认地点，或者发出提示告诉用户怎么正确的输入命令,这些都是比较友好的方式.</p>
<p>以下为部分的策略代码.</p>
```python
        cut_list = jieba.lcut(text)
        reply = u'你可以这么问我：拖鞋杭州到北京的机票'
        if u'到' in cut_list:
            index = cut_list.index(u'到')
            if index > 0:
                depart = cut_list[index - 1]
                destination = cut_list[index + 1]
                reply = '%s %s' % (depart, destination)
```
<p>今天的随笔就写到这里，很多大量的思想以及代码无法在一篇文章中直接表达，所以我都是挑一些重点来记录，如果有留意的小伙伴，欢迎找我一起探讨.</p>

